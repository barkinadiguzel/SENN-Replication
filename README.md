# ðŸ¥ SENN-Replication â€“ Self-Explaining Neural Networks with Intrinsic Interpretability

This repository provides a **PyTorch-based replication** of  
**Self-Explaining Neural Networks â€“ Alvarez-Melis & Jaakkola, NeurIPS 2018**.

The focus is **translating the theoretical SENN framework into a clean, modular, and practical architecture**,  
rather than chasing benchmark SOTA results.

- Learns **interpretable concepts** as internal reasoning units ðŸ§   
- Produces **intrinsic explanations by design** (not post-hoc) ðŸªž  
- Enforces **faithfulness and stability via gradient regularization** ðŸ§­  
- Modular & lightweight, **plug-and-play for any encoder backbone** ðŸ› ï¸  

**Paper reference:** [Towards Robust Interpretability with Self-Explaining Neural Networks â€“ Alvarez-Melis & Jaakkola, 2018](https://arxiv.org/abs/1806.07538) ðŸ“„

---

## ðŸ§  Overview â€“ Self-Explaining Pipeline

![SENN Overview](images/figmix.jpg)

The core idea:

> The model must not only predict â€” it must **explain itself while predicting**.

Instead of learning a black-box mapping  
$x \rightarrow y$,  
the model is structured as:

$$
x \;\longrightarrow\; h(x), \theta(x) \;\longrightarrow\; y
$$

Where:
- $x$ = raw input (image, signal, etc.)
- $h(x)$ = vector of interpretable concepts  
- $\theta(x)$ = relevance scores for each concept  
- $y$ = final prediction  

The prediction is computed as:

$$
f(x) = g(\theta(x) \odot h(x))
$$

This creates a **self-explaining model**:
> Every prediction comes with its own explanation.

---

## ðŸ§® SENN Computation â€“ Math Essentials

### Concept Encoding (x â†’ h)

Given input $x$ and encoder $E$:

$$
z = E(x), \quad h(x) = C(z)
$$

Where:
- $z \in \mathbb{R}^d$ is latent feature representation  
- $h(x) \in \mathbb{R}^k$ is concept vector  

Each dimension corresponds to one interpretable concept.

### Relevance Parametrization (x â†’ Î¸)

A second network produces **input-dependent relevance scores**:

$$
\theta(x) = P(z)
$$

Where:
- $\theta(x) \in \mathbb{R}^k$
- Each $\theta_i(x)$ measures how important concept $h_i(x)$ is for this specific input.

### Aggregation (h, Î¸ â†’ y)

Prediction is computed via an additive monotone aggregator:

$$
f(x) = g(\theta(x) \odot h(x)) = \sum_{i=1}^k \theta_i(x) h_i(x)
$$

This preserves interpretability:
- Each concept contributes independently
- No entangled interactions
- Signs and magnitudes remain meaningful

---

## ðŸ§­ Stability Regularization 

SENN enforces that explanations are **not cosmetic**.

The relevance scores $\theta(x)$ must reflect the true sensitivity of the model.

This is enforced by matching:

$$
\nabla_x f(x) \;\approx\; J_h(x) \cdot \theta(x)
$$

Where:
- $\nabla_x f(x)$ = true gradient of the model (real behavior)
- $J_h(x)$ = Jacobian of the concept encoder
- $\theta(x)$ = explanation vector

Stability loss:

$$
\mathcal{L}_{\text{stability}} = \left\| \nabla_x f(x) - J_h(x)\theta(x) \right\|^2
$$

This acts as a **truth constraint**:
> The explanation must match the modelâ€™s actual reasoning.

---

## ðŸ§  What the Model Enables

- Intrinsic explanations (not post-hoc)
- Local linear reasoning with adaptive coefficients
- Faithful concept attribution
- Stability across similar inputs
- Transparent decision pathways
- Human-readable model behavior

The model is **interpretable by construction**.

---

## ðŸ“¦ Repository Structure

```bash
SENN-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ backbone/
â”‚   â”‚   â””â”€â”€ encoder.py          # Input â†’ latent feature extractor
â”‚   â”‚
â”‚   â”œâ”€â”€ concepts/
â”‚   â”‚   â””â”€â”€ concept_encoder.py  # h(x): latent â†’ concept vector
â”‚   â”‚
â”‚   â”œâ”€â”€ relevance/
â”‚   â”‚   â””â”€â”€ parametrizer.py     # Î¸(x): latent â†’ relevance scores
â”‚   â”‚
â”‚   â”œâ”€â”€ aggregator/
â”‚   â”‚   â””â”€â”€ aggregator.py       # g(Î¸ âŠ™ h) â†’ prediction
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ senn.py             # Full x â†’ h,Î¸ â†’ y + explanation
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚   â””â”€â”€ senn_loss.py        # Prediction + stability + concept loss
â”‚   â”‚
â”‚   â”œâ”€â”€ explanation/
â”‚   â”‚   â””â”€â”€ explain.py         # Explanation extraction
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg             # SENN overview figure
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ðŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
